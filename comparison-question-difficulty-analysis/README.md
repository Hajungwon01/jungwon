# Comparison Question Difficulty Analysis

> 대규모 언어 모델(LLM)이 Comparison 질문에서 왜 더 자주 실패하는지를  
> 정량·정성 분석을 통해 규명한 개인 연구 프로젝트입니다.

---

## Overview
본 프로젝트는 LLM이 **비교형(Comparison) 질문**에서  
다른 유형의 질문 대비 일관되게 낮은 성능을 보이는 원인을 분석하기 위해 수행한  
개인 주도 연구입니다.

단순히 정답률을 비교하는 데서 그치지 않고,  
**어떤 단계에서 추론이 붕괴되는지**,  
**어떤 질문 속성이 난이도를 높이는지**를 중심으로  
모델의 오류 패턴을 체계적으로 분석하는 것을 목표로 하였습니다.

---

## Research Questions
본 연구는 다음 질문들에 답하고자 하였습니다.

- Comparison 질문은 왜 Bridge 질문보다 어려운가?
- LLM은 비교 과정 중 어떤 단계에서 오류를 범하는가?
- 정답을 맞힌 경우에도 추론 과정은 안정적인가?

---

## Approach
연구 목적에 맞게 다음과 같은 분석 파이프라인을 설계·구현하였습니다.

1. **질문 유형 분해**
   - Comparison 질문을 sub-question 단위로 분해
   - 최종 정답과 중간 추론 단계 성능을 분리 평가

2. **정량 분석**
   - EM / F1 기반 성능 비교
   - sub-question 정오답 조합 패턴 분석

3. **정성 분석**
   - 정답은 맞았으나 추론이 붕괴된 사례 분석
   - 비교 대상 혼동, 속성 선택 오류 등 오류 유형 분류

---

## Key Findings
분석 결과 다음과 같은 주요 관찰을 확인하였습니다.

- Comparison 질문에서는  
  중간 추론 단계가 실패했음에도 최종 정답을 맞히는 경우가 빈번히 발생
- 이는 LLM이 비교 추론을 수행했다기보다  
  표면적 패턴이나 편향에 의존했을 가능성을 시사
- 비교 대상 속성이 유사하거나 맥락이 불명확할수록  
  오류 발생률이 유의미하게 증가

---

## My Role
- 연구 질문 정의 및 실험 설계
- 분석 파이프라인 구현
- 정량·정성 분석 수행
- 결과 해석 및 인사이트 도출

본 프로젝트는 문제 정의부터 분석, 해석까지 전 과정을 단독으로 수행하였습니다.

---

## Project Structure
```
.
├─ data/            # 분석용 데이터 (비공개, 샘플만 공개)
├─ src/             # 분석 로직 및 평가 코드
├─ experiments/     # 실험 결과 요약
├─ scripts/         # 실행 스크립트
└─ README.txt
```

---

## Why It Matters
본 연구는 멀티홉 QA에서 높은 정답률이  
반드시 안정적인 추론 능력을 의미하지는 않는다는 점을 보여줍니다.

이는 LLM 평가에서 단순 EM/F1 지표만으로는  
모델의 실제 추론 능력을 과대평가할 수 있음을 시사하며,  
비교형 질문에 대한 보다 정교한 평가 기준의 필요성을 제기합니다.

---

## Notes
- 본 프로젝트는 개인 연구로 수행되었습니다.
- 사용된 데이터는 공개 범위에 맞게 일부만 포함되어 있습니다.
